\documentclass[../tech_report_1.tex]{subfiles}
\graphicspath{{img/}{../img/}}

\begin{document}

\part*{Hierarchical clustering}

For our lab, we want to develop the most efficient way to group data
based based on a measure of dissimilarity. We explored the topic of
hierarchical clustering which is used in data mining and statistics.
It is visualized useing a dendogram to show the organization and relationship
among clusters. Hierarchical clustering can be implemented in two
ways: divisive and agglomerative. We will elaborate on these two variants
in this paper to compare the its algorithmic complexity, or how much
resources are needed when computed, and the different types of cluster
linkage criterion. Further information on cluster analysis can be
found in \cite{ClusterAnalysis}.

\section*{Divisive and Agglomerative}

Divise clustering is also known as a ``top down'' method. This takes
in a data set as one large cluster. Then as it moves down the hierarchy,
it recusively splits until it reaches the leaves of the tree, or each
observation is its own singleton cluster. 

Agglomerative clustering can be thought to do the opposite of divisive
clustering and is known as a ``bottom up'' strategy. It takes in
a data set and it initially looks at each observation as its own singleton
cluster. Then, based on its inkage criterion, it will merge recursively
to form new clusters until all the data are in one large cluster.
The dendrogram is used in both cases to record and visiualize each
cluster merge and their distance to each other.


\section*{Cluster Dissimilarity Measurements}

In forming clusters, a measurement must be made to determine which
clusters split or merge in regards to divisive or agglomerative clustering,
respectively. There are many different ways to measure dissimilarity
but this paper will explore single linkage, complete linkage, and
centroid linkage. Each of the measurements below refer to ways in
measuring distances between $k$ number of clusters $c_{i=1\dots k}$
in relationship with specific points $x_{i=1\dots k}$ in those clusters.


\subsection*{Single Linkage}

Single linkage takes the distance between the closest observations,
or minimum distance, in each cluster and merges the those with the
shortest distance. This works well for data sets that desire to cluster
together long chained data points. The equation is defined as
\[
D(c_{1},c_{2})=\underset{x_{1}\in c_{1},x_{2}\in c_{2}}{min}D(x_{1},x_{2})
\]



\subsection*{Complete Linkage}

Complete linkage is similar to single linkage. It takes the distance
between the farthest observations, or the maximum distance, in each
cluster. The clusters that have the shortest measured distance is
merged together. This measurement is ideal for data that is sphereically
organized relative to each other. The equation is defined as
\[
D(c_{1},c_{2})=\underset{x_{1}\in c_{1},x_{2}\in c_{2}}{max}D(x_{1},x_{2})
\]



\subsection*{Centroid Linkage}

Using centroid linkage involves taking the average of all points in
a cluster and assigning that value as the mean cenroid. Then we find
the centroids with the shortest distances and merge them together.
Centroid linkage is defined as
\[
D(c_{1},c_{2})=D((\frac{1}{|c_{1}|}\sum_{x\in c_{1}}\overset{\rightarrow}{x}),(\frac{1}{|c_{2}|}\sum_{x\in c_{2}}\overset{\rightarrow}{x}))
\]



\section*{Our Implementation}

We developed both hierarchical divisive and agglomerrative clustering
functions to test which would organize data the fastest. In our particular
application, instead of flat clustering, we had to cluster over the
curved surface of a sphere. In order to do this, we utilized the SPKmeans
function \cite{SphericalKMeans}. We also chose our linkage criterion
to be based on centroid linkage.

Essentially, we developed the divisive clustering by following the
intuition mentioned above. For our agglomerative clustering implementation,
we did a slightly different variation hoping its run time would be
slightly better than the traditional method. In our code, intuitively,
we initiated each point as its own cluster with its own centroid.
Then we recursively merged two clusters together based on centroid
linkage to form a new cluster with its own cluster centroid everytime.
This process stops when there is a single point to represent the entire
data's average centroid.

Complexity determines how well the algorithm performs in real time
as the problem size increases. We tested with a relatively small,
three dimensional data set of 150 points. This way, we can later expand
it to work with large data sets that are multidemnsional. Since the
SPKmeans is the function that takes the longest to perform, it is
our leading function to determine complexity. In the situtations of
divisive and agglomerative, we input different cluster values into
SPKmeans which determines how often the function gets called. In the
divisive method, we asked SPKmeans to return two clusters everytime
it is given a data set. This means the number of times we call SPKmeans
is based on how many nodes we have which is $m=2n-1$ where $n$ is
the number of data points. For agglomertive, we asked SPKmeans to
return the $n/2$ clusters. This means means we had to call SPKmeans
at every level which is $l=ceiling(log_{2}n)$.


\subsection*{Algorithm complexity analysis}

Let $n$ be the number of data points, $k$ be the number of clusters (number of means), $d$ be the dimensionality of the data, and $i$ be the number of iterations until convergence. The runtime of the $k$-means algorithm (including spherical k-means) is $O(nkdi)$ (TODO: CITE).

Let us assume that the cost of initializing the means and splitting empty clusters is negligible, i.e. $O(1)$. In practice, the two are related: we find that explicitly picking good starting means for the agglomerate algorithm (e.g. randomly, or by maximizing distance between the means) makes the probability of empty clusters small, whereas it becomes excessive and unnecessary for the divisive algorithm. TODO: We explain this in a later section.

We will also assume that $d$ and $i$ are constants. Though in degenerate data sets $i$ can be an exponential function of $n$, in practice $i$ is a small constant on datasets with clustering structuring (CITE: WIKI?).

Let us also assume that we choose a branching factor of 2, i.e. a binary hierarchical tree structure. The following analysis does not depend on the branching factor, assuming it is constant.

\begin{theorem}

\textbf{Agglomerative complexity} $$T_{aggl}(n) = \Theta(n^2di)$$

\end{theorem}

\textbf{Proof.} Our agglomerate algorithm has a recurrence relation of the following form:

$$ T_{aggl}(n) = T(\frac{n}{2}) + nkdi $$

for some constant $c$, since we divide the problem size into two for every recursive call. But we choose $k=\frac{n}{2}$ each time, since the number of means we choose to cluster with is also half of $n$.

We use the master theorem (TODO: CITE) to solve this recurrence relation:

\begin{table}[ht]
\centering
\begin{tabular}{l || c }
\hline
\textbf{Master theorem variable} & \textbf{Value} \\
\hline
$a$ & 1 \\
$b$ & 2 \\
$f(n)$ & $\frac{n^2di}{2}$ \\
$c$ & 2 \\
$log_b(a)$ & 0 \\
\hline
\end{tabular}
\end{table}

This satisfies case 3 of the master theorem:

$$ f(n) \in \Omega(n^c) \text{ s.t. } c > log_b(a) $$

since $f(n) \in \Omega(n^2) \text{ s.t. } c = 2 > log_b(a) = 0$, and

$$ af(\frac{n}{b}) \leq k_0 f(n) \text{ for some } k_0 < 1 \text{ and sufficiently large } n$$

since when  $k_0 = \frac{1}{4}$, $\left\{f(\frac{n}{2}) = (\frac{1}{4}) \frac{n^2di}{2}\right\} \leq \left\{k_0f(n) = (\frac{1}{4}) \frac{n^2di}{2}\right\}$

and so, by the master theorem: $ T_{aggl}(n) = \Theta(n^2di)$. \qedsymbol


\begin{theorem} 

\textbf{Divisive complexity}

$$ T_{div}(n) = \Theta(n\log(n)di) $$

\end{theorem}

\textbf{Proof.} Our divisive algorithm has a recurrence relation of the following form:

$$ T_{div}(n) = 2T(\frac{n}{2}) + nkdi $$

for some constant $c$, since we divide the problem size into two for every recursive call and we have two subproblems each time. But we choose $k=2$ each time, since the number of means we choose to cluster with is always the branching factor.

We use the master theorem (TODO: CITE) to solve this recurrence relation:

\begin{table}[ht]
\centering
\begin{tabular}{l || c }
\hline
\textbf{Master theorem variable} & \textbf{Value} \\
\hline
$a$ & 2 \\
$b$ & 2 \\
$f(n)$ & ${n(2)di}$ \\
$c$ & 1 \\
$log_b(a)$ & 1 \\
\hline
\end{tabular}
\end{table}

This satisfies case 2 of the master theorem:

$$ f(n) \in \Theta(n^c\log^k_0n) \text{ s.t. } c = log_b(a) $$

by letting $k_0=0$, since $f(n\log^0(n)) = f(n) \in \Theta(n) \text{ s.t. } c = 1 = log_b(a) = 1$.

and so, by the master theorem: $ T_{div}(n) = \Theta(n\log(n)di)$. \qedsymbol

\end{document}
