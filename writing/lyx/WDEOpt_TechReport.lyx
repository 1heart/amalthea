#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Wavelet Density Estimator Optimization
\end_layout

\begin_layout Author
Yixin Lin, Glizela Taino, Mark Moyou, and Dr.
 Adrian Peter
\end_layout

\begin_layout Standard
Using Wavelet Density Estimation in shape matching was explored by Dr.
 Adrian M.
 Peter and Dr.
 Anand Rangarajan in 
\begin_inset CommandInset citation
LatexCommand cite
key "peter2008WDE"

\end_inset

.
 It estimates probability density functions of sample data whose function
 are unknown.
 It does this by using Wavelets theory and applying it to density estimation.
 It's used in applications such as 1D, 2D, and 3D shape matching.
 The coefficients that are used to scale the basis functions act as feature
 representations for a shape.
 These feature representations are constrained to one which conveniently
 map them onto a unit hypersphere.
 The geometry of the hypersphere can be easily manipulated for shape retrieval,
 in effectively recognizing similar and dissimilar shapes based on distance
 between coefficient vectors.
 The Wavelet Density Estimator code they developed is essential in extracting
 shape densities as features representations for various shapes.
 Computation involved for a sample point set can be very complicated.
 Therefore, the code is designed to execute the necessary computation.
 However, the issue with this code is the time it takes to process large
 databases.
 Databases such as Brown, Swedish Leaf, and MPEG7 have 99, 1125, an 1400
 shapes, respectively, and can take days to process even with a relatively
 low resolution.
 This is due to the design and structure of the implementation.
 The code is designed to perform superfluous and redundant computations
 to solve for basis functions
\backslash
 values for every sample point individually.
 As a solution to this problem we only calculated the basis functions for
 relevant sample points under specified basis functions with the hope to
 cut computation and time considerably.
 As wavelet density coefficients prove to be distinctly representative of
 their shape and can be useful in shape retrieval, the computational time
 to extract these coefficients become ever more important.
 If we wish to see timely growth in this area of research, hours for coefficient
 extraction for a database will not help in achieving this outlook.
 If we wish to see wavelet density coefficients used more commonly as represent
ion, the wavelet density estimator must work more efficiently.
 In this report we will get a basic understanding of wavelets and how it's
 used in density estimation.
 Then we will delve into the code to expand its inefficiency, explain our
 solution to the problem, and lastly we will go over the very promising
 results from the optimization.
 
\end_layout

\begin_layout Section*
Wavelet Density Estimation
\end_layout

\begin_layout Standard
In order to create our feature representation we have to estimate the probability
y density function of sample points that make up a shape.
 Wavelets act as the basis function of the function space 
\begin_inset Formula $L^{2}$
\end_inset

.
 An 
\begin_inset Formula $L^{2}$
\end_inset

 function is a function that consists of real numbers that is squared integrable
, or finite.
 Probability density functions fall under this function space, therefore
 allowing us to use wavelets for density estimation using a linear combination
 of these basis functions.
 The equation for 1D density estimation is 
\begin_inset Formula 
\begin{equation}
p(x)=\underset{j_{o},k}{\sum}\alpha_{j_{o},k}\phi_{j_{o},k}(x)+\underset{j\geq j_{o},k}{\overset{j_{1}}{\sum}}\beta_{j,k}\psi_{j,k}(x)\label{eq:WDE}
\end{equation}

\end_inset

where 
\begin_inset Formula $\phi(x)$
\end_inset

 and 
\begin_inset Formula $\psi(x)$
\end_inset

 are the scaling and wavelet basis functions, also known as the 
\begin_inset Quotes eld
\end_inset

father
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

mother
\begin_inset Quotes erd
\end_inset

 wavelets, respectively.
 The functions can also be set to some starting resolution level that determines
 its dilation and contraction.
 To extend to multiple resolutions, we incorporate the wavelet basis function
 with a starting, 
\begin_inset Formula $j_{o}$
\end_inset

, and stopping, 
\begin_inset Formula $j$
\end_inset

, resolution level.
 In order to form a basis, we need to make multiple copies of the function
 over different translates, 
\begin_inset Formula $k$
\end_inset

.
 These functions are scaled by their scaling and wavelet basis function
 coefficients, 
\begin_inset Formula $\alpha_{j_{o},k}$
\end_inset

 and 
\begin_inset Formula $\beta_{j,k}$
\end_inset

.
 To retain properties of true densities, such as non-negativity and should
 integrate to one, we compute the square root of the probability density function
 
\begin_inset Formula 
\begin{equation}
\sqrt{p(x)}=\underset{j_{o},k}{\sum}\alpha_{j_{o},k}\phi_{j_{o},k}(x)+\underset{j\geq j_{o},k}{\overset{j_{1}}{\sum}}\beta_{j,k}\psi_{j,k}(x)\label{eq:sqrtWDE}
\end{equation}

\end_inset

Since we optimized for multi-resolution 2D wavelet density estimation, we
 consider the second dimension with 
\begin_inset Formula $\mathbf{x}=(x_{1},x_{2})\in\mathbb{R}^{2}$
\end_inset

 and 
\series bold

\begin_inset Formula $\mathbf{k}=(k_{1},k_{2})\in\mathbb{Z}^{2}$
\end_inset


\series default
.
 We also extend the equation to 
\begin_inset Formula 
\begin{equation}
\sqrt{p(\mathbf{x})}=\underset{j_{o},\mathbf{k}}{\sum}\alpha_{j_{o},\mathbf{k}}\phi_{j_{o},\mathbf{k}}(\mathbf{x})+\underset{j\geq j_{o},\mathbf{k}}{\overset{j_{1}}{\sum}}\overset{3}{\underset{w=1}{\sum}}\beta_{j,\mathbf{k}}^{w}\psi_{j,\mathbf{k}}^{w}(\mathbf{x})\label{eq:sqrtWDE2Dmultires}
\end{equation}

\end_inset

where we calculate for the basis functions by using the tensor product method
 
\begin_inset Formula 
\[
\phi_{j_{o},\mathbf{k}}(\mathbf{x})=2^{j_{o}}\phi(2^{j_{o}}x_{1}-k_{1})\phi(2^{j_{o}}x_{2}-k_{2})
\]

\end_inset


\begin_inset Formula 
\[
\psi_{j,\mathbf{k}}^{1}(\mathbf{x})=2^{j}\phi(2^{j}x_{1}-k_{1})\psi(2^{j}x_{2}-k_{2})
\]

\end_inset


\begin_inset Formula 
\[
\psi_{j,\mathbf{k}}^{2}(\mathbf{x})=2^{j}\psi(2^{j}x_{1}-k_{1})\phi(2^{j}x_{2}-k_{2})
\]

\end_inset


\begin_inset Formula 
\begin{equation}
\psi_{j,\mathbf{k}}^{3}(\mathbf{x})=2^{j}\psi(2^{j}x_{1}-k_{1})\psi(2^{j}x_{2}-k_{2})\label{eq:multiRes2D}
\end{equation}

\end_inset

A mathematical advantage in using wavelets is its compact support, meaning
 they are nonzero on a small domain.
 Any point falling out of the basis function's support does not contribute
 to the basis function value at that translation.
 Intuitively, the scaling and wavelet basis function coefficients determine
 the height of its basis function.
 Therefore, our goal is to obtain the coefficients which uniquely define
 the probability density function of a shape and use as a feature representation
 in shape retrieval.
 In order to calculate coefficients we use the following equations 
\begin_inset Formula 
\begin{equation}
\alpha_{j_{o},\mathbf{k}}=\bigg(\frac{1}{N}\bigg)\overset{N}{\underset{i=1}{\sum}}\frac{\phi_{j_{o},\mathbf{k}}(\mathbf{x})}{\sqrt{p(\mathbf{x})}}\label{eq:scalingCoeff}
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
\beta_{j_{o},k}=\bigg(\frac{1}{N}\bigg)\underset{i=1}{\overset{N}{\sum}}\frac{\psi_{j,\mathbf{k}(\mathbf{x})}}{\sqrt{p(\mathbf{x})}}\label{eq:waveletCoeff}
\end{equation}

\end_inset

As we estimate the coefficients of our probability density function, we use
 a loss function to determine whether our coefficients are as close to optimal
 as possible.
 To do this we try to minimize a negative loglikelihood objective function
 with respect to the coefficients 
\begin_inset Formula 
\begin{equation}
-\log p(X;\{\alpha_{j_{o},k}\beta_{j,k}\})=-\frac{1}{N}\underset{i=1}{\overset{N}{\sum}}\log\Bigg[\underset{j_{o},k}{\sum}\alpha_{j_{o},k}\phi_{j_{o},k}(x_{i})+\underset{j\geq j_{o},k}{\overset{j_{1}}{\sum}}\beta_{j,k}\psi_{j.k}(x_{i})\Bigg]^{2}\label{eq:costFunction}
\end{equation}

\end_inset

where 
\begin_inset Formula $X=\{x_{i}\}_{i=1}^{N}$
\end_inset

.
 And in order to determine the direction of the gradient descent we compute
 the following
\begin_inset Formula 
\begin{equation}
-2\bigg(\frac{1}{N}\bigg)\sum\frac{\phi_{j_{o},\mathbf{k}}(\mathbf{x})}{\sum\alpha_{j_{o},\mathbf{k}}\phi_{j_{o},\mathbf{k}}(\mathbf{x})}\label{eq:gradient}
\end{equation}

\end_inset

In order for maintain essential properties of density estimation, we constrain
 the coefficients to one
\begin_inset Formula 
\begin{equation}
\underset{j_{o},k}{\sum}\alpha_{j_{o},k}^{2}+\underset{j\geq j_{o},k}{\overset{j_{1}}{\sum}}\beta_{j,k}^{2}=1\label{eq:constraint}
\end{equation}

\end_inset

This constraint conveniently maps our feature representation into a unit
 hypersphere which allows us to take advantage of its geometric properties
 to signify similarity and dissimilarity between shapes in shape retrieval.
 
\end_layout

\begin_layout Section*
Wavelet Density Estimation Optimization
\end_layout

\begin_layout Standard
Now that we have an overview of wavelet density estimation and the equations
 used, we can delve into our optimization.
 Our ultimate goal is to estimate a 2D density function on a given point
 set shape representation.
 We then use the coefficients that uniquely characterize the density function
 as our feature vector.
 Originally, for a data base such as MPEG7 made up of 1400 shapes with a
 domain of [-0.5,0.5], resolution level 2 to 3, and 1344 translates it would
 take roughly 16.8 hours.
 This is a particularly low resolution and if raised we would expect an
 even slower runtime.
 We want to optimize this code for speed.
 The bottleneck occurs when calculating the initial coefficients and negative
 log likelihood cost value with its gradient.
 
\end_layout

\begin_layout Subsection*
Initializing Coefficients
\end_layout

\begin_layout Standard
This function sets out to calculate equations (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:scalingCoeff"

\end_inset

) and (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:waveletCoeff"

\end_inset

) to compute for estimated initial coefficients that will be updated later
 in the code for the wavelet density estimation.
 The time spent for this calculation with the same parameters mentioned
 above is roughly around 1.8 of the 16.8 hours.
 
\end_layout

\begin_layout Standard
The reason for this bottleneck is because of the way the code executes computation.
 Given a point set shape representation, it covers the shape with basis
 functions at each translation.
 Each function has a coefficient that needs to be calculated.
 The problem is that the code computes equations (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:multiRes2D"

\end_inset

) for a 
\emph on
single point over all translations
\emph default
 using a MATLAB function called Kronecker Tensor Product [CITE].
 This means that if we have 1344 translations, it would perform 1344 operations.
 If a shape is made up of 4007 sample points, then it would be performing
 a total of 5,385,408 operations for a single shape.
 This calls for an excess amount of redundant computations since most of
 the scaling and wavelet basis functions for a single point over all translation
s return the value of zero.
 It would return zero because a specified observed sample point does not
 contribute to most basis functions, or does not exist under the compact
 support of the basis function at specific translations.
 Therefore, our solution was to find an approach that would require less
 computation and, overall, work more effectively and efficiently.
 
\end_layout

\begin_layout Standard
Instead of looping through each point and finding which basis functions
 it falls under, we looped through basis functions along the horizontal
 direction, evaluated its basis function value (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:multiRes2D"

\end_inset

) based on the points fall under its support, and placed them into a matrix
 that holds these values.
 We then store this matrix to be used for later optimized computation (for
 the negative log likelihood cost value and gradient).
 Once we have this matrix of values, this allows us to evaluate the basis
 function coefficients with the relevant points under each translation using
 equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:scalingCoeff"

\end_inset

) and (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:waveletCoeff"

\end_inset

).
 
\end_layout

\begin_layout Subsection*
Negative Log Likelihood
\end_layout

\begin_layout Standard
The second area where the bottleneck occurs is in computing for the negative
 log likelihood cost value (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:costFunction"

\end_inset

) and the function's gradient (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:gradient"

\end_inset

).
 We use these equations to check whether we have found the most optimal
 coefficient values and which direction in which to descend.
 The original time spent for this computation was around 2 of the 16.8 hours.
 
\end_layout

\begin_layout Standard
The problem with the code was similar to the execution of initialize coefficient
s.
 It would take a single point and calculate the basis function over all
 translations resulting in needless computation.
 It does this to attain a matrix of the basis function values to perform
 the appropriate operations to solve for the cost value and gradient.
 
\end_layout

\begin_layout Standard
Since we already performed most of the heavy computation to attain this
 matrix in initializing coefficients, our solution was to simply pass in
 the needed matrix of basis function values for each translation and perform
 the proper computations.
 This effectively solves for the negative log likelihood cost value and
 gradient while ultimately eliminating loops.
 
\end_layout

\begin_layout Section*
Results
\end_layout

\begin_layout Standard
As mentioned above, our goal is to estimate a density function of a 2D shape
 using wavelets.
 This allows us to extract the coefficients as our feature representation
 to be used in shape retrieval on a unit hypersphere.
 However, with MPEG7 containing 1400 shapes, domain [-0.5,0.5], resolution
 level 2 to 3, and 1344 translates it would take about 16.8 hours to calculate
 the coefficients.
 After our optimization implementation to the two areas where the code bottlenecks
 , we yield fantastic results.
\end_layout

\begin_layout Standard
The original time it took the code to calculate for only the initial coefficient
s was about 1.8 hours.
 After our optimization, the run time to compute the initial coefficients
 cut all the way down to 0.13 hours, or about eight minutes.
 Ultimately, the computation runs 92.8% faster.
 As for calculations for the negative log likelihood cost value and gradient,
 our optimization shaved the lines of code from 54 lines to 21 lines, ridding
 it of loops.
 We essentially do away with 61% of lines of code.
 The run time was promising as well.
 To calculate the cost value and gradient for seven iterations, computation
 time went from 13.9 hours to 0.089 hours, or around 5 minutes for the entire
 dataset.
 The optimized computation runs 99% faster.
 Overall, to estimate the wavelet densities of a database of 2D shapes with
 the optimized code under the specified parameters would take 0.3 hours,
 or 17.8 minutes, running 98% faster.
\end_layout

\begin_layout Standard
With this optimization, we hope that we can spend less time waiting for
 the wavelet density coefficient values to be estimated and more time in
 developing new and better methods of shape retrieval using wavelet density
 coefficients as feature vectors.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "/Users/glizelataino/Documents/references"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
