#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Hierarchical Clustering
\end_layout

\begin_layout Author
Yixin Lin, Glizela Taino
\end_layout

\begin_layout Date
May 23, 2016
\end_layout

\begin_layout Abstract
For our lab, we want to develop the most efficient way to group data based
 based on a measure of dissimilarity.
 We explored the topic of hierarchical clustering which is used in data
 mining and statistics.
 It is visualized useing a dendogram to show the organization and relationship
 among clusters.
 Hierarchical clustering can be implemented in two ways: divisive and agglomerat
ive.
 We will elaborate on these two variants in this paper to compare the its
 algorithmic complexity, or how much resources are needed when computed,
 and the different types of cluster linkage criterion.
 Further information on cluster analysis can be found in 
\begin_inset CommandInset citation
LatexCommand cite
key "ClusterAnalysis"

\end_inset

.
\end_layout

\begin_layout Part*
Divisive and Agglomerative
\end_layout

\begin_layout Standard
Divise clustering is also known as a 
\begin_inset Quotes eld
\end_inset

top down
\begin_inset Quotes erd
\end_inset

 method.
 This takes in a data set as one large cluster.
 Then as it moves down the hierarchy, it recusively splits until it reaches
 the leaves of the tree, or each observation is its own singleton cluster.
 
\end_layout

\begin_layout Standard
Agglomerative clustering can be thought to do the opposite of divisive clusterin
g and is known as a 
\begin_inset Quotes eld
\end_inset

bottom up
\begin_inset Quotes erd
\end_inset

 strategy.
 It takes in a data set and it initially looks at each observation as its
 own singleton cluster.
 Then, based on its inkage criterion, it will merge recursively to form
 new clusters until all the data are in one large cluster.
 The dendrogram is used in both cases to record and visiualize each cluster
 merge and their distance to each other.
\end_layout

\begin_layout Section*
Cluster Dissimilarity Measurements
\end_layout

\begin_layout Standard
In forming clusters, a measurement must be made to determine which clusters
 split or merge in regards to divisive or agglomerative clustering, respectively.
 There are many different ways to measure dissimilarity but this paper will
 explore single linkage, complete linkage, and centroid linkage.
 Each of the measurements below refer to ways in measuring distances between
 
\begin_inset Formula $k$
\end_inset

 number of clusters 
\begin_inset Formula $c_{i=1\dots k}$
\end_inset

 in relationship with specific points 
\begin_inset Formula $x_{i=1\dots k}$
\end_inset

 in those clusters.
\end_layout

\begin_layout Subsection*
Single Linkage
\end_layout

\begin_layout Standard
Single linkage takes the distance between the closest observations, or minimum
 distance, in each cluster and merges the those with the shortest distance.
 This works well for data sets that desire to cluster together long chained
 data points.
 The equation is defined as
\begin_inset Formula 
\[
D(c_{1},c_{2})=\underset{x_{1}\in c_{1},x_{2}\in c_{2}}{min}D(x_{1},x_{2})
\]

\end_inset


\end_layout

\begin_layout Subsection*
Complete Linkage
\end_layout

\begin_layout Standard
Complete linkage is similar to single linkage.
 It takes the distance between the farthest observations, or the maximum
 distance, in each cluster.
 The clusters that have the shortest measured distance is merged together.
 This measurement is ideal for data that is sphereically organized relative
 to each other.
 The equation is defined as
\begin_inset Formula 
\[
D(c_{1},c_{2})=\underset{x_{1}\in c_{1},x_{2}\in c_{2}}{max}D(x_{1},x_{2})
\]

\end_inset


\end_layout

\begin_layout Subsection*
Centroid Linkage
\end_layout

\begin_layout Standard
Using centroid linkage involves taking the average of all points in a cluster
 and assigning that value as the mean cenroid.
 Then we find the centroids with the shortest distances and merge them together.
 Centroid linkage is defined as
\begin_inset Formula 
\[
D(c_{1},c_{2})=D((\frac{1}{|c_{1}|}\sum_{x\in c_{1}}\overset{\rightarrow}{x}),(\frac{1}{|c_{2}|}\sum_{x\in c_{2}}\overset{\rightarrow}{x}))
\]

\end_inset


\end_layout

\begin_layout Section*
Our Implementation
\end_layout

\begin_layout Standard
We developed both hierarchical divisive and agglomerrative clustering functions
 to test which would organize data the fastest.
 In our particular application, instead of flat clustering, we had to cluster
 over the curved surface of a sphere.
 In order to do this, we utilized the spherical $k$-means function 
\begin_inset CommandInset citation
LatexCommand cite
key "SphericalKMeans"

\end_inset

.
 We also chose our linkage criterion to be based on centroid linkage.
\end_layout

\begin_layout Standard
Essentially, we developed the divisive clustering by following the intuition
 mentioned above.
 For our agglomerative clustering implementation, we did a slightly different
 variation hoping its run time would be slightly better than the traditional
 method.
 In our code, intuitively, we initiated each point as its own cluster with
 its own centroid.
 Then we recursively merged two clusters together based on centroid linkage
 to form a new cluster with its own cluster centroid everytime.
 This process stops when there is a single point to represent the entire
 data's average centroid.
\end_layout

\begin_layout Standard
Complexity determines how well the algorithm performs in real time as the
 problem size increases.
 We tested with a relatively small, three dimensional data set of 150 points.
 This way, we can later expand it to work with large data sets that are
 multidemnsional.
 Since the spherical $k$-means is the function that takes the longest to perform, it
 is our leading function to determine complexity.
 In the situtations of divisive and agglomerative, we input different cluster
 values into spherical $k$-means which determines how often the function gets called.
 In the divisive method, we asked spherical $k$-means to return two clusters everytime
 it is given a data set.
 This means the number of times we call spherical $k$-means is based on how many nodes
 we have which is 
\begin_inset Formula $m=2n-1$
\end_inset

 where 
\begin_inset Formula $n$
\end_inset

 is the number of data points.
 For agglomertive, we asked spherical $k$-means to return the 
\begin_inset Formula $n/2$
\end_inset

 clusters.
 This means means we had to call spherical $k$-means at every level which is 
\begin_inset Formula $l=ceiling(log_{2}n)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "references"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
